{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPZOWfKXvgC1MBbUG387hyE"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "# !gdown 1-S_DPVzrTNdzyRqb5hSrsmLk2c0-6tpk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kkg0F2tSImJS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1760083228037,
     "user_tz": -240,
     "elapsed": 2692,
     "user": {
      "displayName": "Karen Nikoghosyan",
      "userId": "03436019369254472076"
     }
    },
    "outputId": "dadb6dde-db00-4fd8-8365-a464f4d44f6d"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-S_DPVzrTNdzyRqb5hSrsmLk2c0-6tpk\n",
      "To: /content/robert_frost.txt\n",
      "\r  0% 0.00/56.3k [00:00<?, ?B/s]\r100% 56.3k/56.3k [00:00<00:00, 55.6MB/s]\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T10:42:35.842329Z",
     "start_time": "2025-10-14T10:42:35.807510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "# P(w1) - Probability distribution for first words in lines\n",
    "# initial_probabilities[word] = count(word as first) / total_lines\n",
    "initial_probabilities = {}\n",
    "# P(wt|wt-1) - First-order transition probabilities\n",
    "# For word given one previous word\n",
    "# first_order_transitions[prev_word][curr_word] = count(prev_word, curr_word) / count(prev_word)\n",
    "first_order_transitions = {}\n",
    "# P(wt|wt-1,wt-2) - Second-order transition probabilities\n",
    "# For word given two previous words\n",
    "# second_order_transitions[(prev_prev_word, prev_word)][curr_word] =\n",
    "#     count(prev_prev_word, prev_word, curr_word) / count(prev_prev_word, prev_word)\n",
    "second_order_transitions = {}\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Removes all punctuation from input string using string.punctuation\n",
    "    Args:\n",
    "        text (str): Input string with possible punctuation\n",
    "    Returns:\n",
    "        str: Clean string without punctuation\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "def add_to_transition_dict(transition_dict, condition, next_word):\n",
    "    \"\"\"\n",
    "    Helper function to build frequency lists in transition dictionaries\n",
    "\n",
    "    Args:\n",
    "        transition_dict (dict): Target transition dictionary to modify\n",
    "        condition: Condition state (word or tuple of words)\n",
    "        next_word: Following word to record\n",
    "    \"\"\"\n",
    "    if condition not in transition_dict:\n",
    "        transition_dict[condition] = []\n",
    "    transition_dict[condition].append(next_word)\n",
    "\n",
    "with open(\n",
    "    r'C:\\Users\\lilit\\PycharmProjects\\nlp-course-2025.1\\Lilit Mnatsakanyan\\data\\tumanyan.txt',\n",
    "    'r',\n",
    "    encoding='utf-8',\n",
    "    errors='replace'\n",
    ") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Training phase - Process input text file line by line\n",
    "for line in text.split('\\n'):\n",
    "    # Clean the line: remove punctuation, convert to lowercase, and split into words\n",
    "    clean_line = remove_punctuation(line.strip().lower())\n",
    "    words = clean_line.split()\n",
    "    line_length = len(words)\n",
    "\n",
    "    # Process each word in the line\n",
    "    for position in range(line_length):\n",
    "        current_word = words[position]\n",
    "\n",
    "        if position == 0:\n",
    "            # First word in line - update initial word frequencies\n",
    "            # Formula: P(w1) = count(w1 as first word) / total number of lines\n",
    "            initial_probabilities[current_word] = initial_probabilities.get(current_word, 0.) + 1\n",
    "        else:\n",
    "            previous_word = words[position - 1]\n",
    "\n",
    "            if position == line_length - 1:\n",
    "                # Last word in line - mark as possible ending\n",
    "                add_to_transition_dict(second_order_transitions,\n",
    "                                       (previous_word, current_word), 'END')\n",
    "\n",
    "            if position == 1:\n",
    "                # Second word in line - update first-order transitions\n",
    "                # Formula: P(wt|wt-1) = count(wt-1, wt) / count(wt-1)\n",
    "                add_to_transition_dict(first_order_transitions, previous_word, current_word)\n",
    "            else:\n",
    "                # All other words - update second-order transitions\n",
    "                # Formula: P(wt|wt-2,wt-1) = count(wt-2,wt-1,wt) / count(wt-2,wt-1)\n",
    "                previous_previous_word = words[position - 2]\n",
    "                add_to_transition_dict(second_order_transitions,\n",
    "                                       (previous_previous_word, previous_word),\n",
    "                                       current_word)\n",
    "\n",
    "# Normalize initial probabilities\n",
    "# P(w1) = count(w1 as first word) / total number of lines\n",
    "total_lines = sum(initial_probabilities.values())\n",
    "for word, count in initial_probabilities.items():\n",
    "    initial_probabilities[word] = count / total_lines\n",
    "\n",
    "\n",
    "def convert_to_probability_dict(word_list):\n",
    "    \"\"\"\n",
    "    Converts a list of words into a probability distribution dictionary\n",
    "\n",
    "    Formula: P(word) = count(word) / total_words\n",
    "\n",
    "    Args:\n",
    "        word_list (list): List of words\n",
    "    Returns:\n",
    "        dict: Dictionary mapping words to their probabilities\n",
    "    Example:\n",
    "        Input: ['cat', 'dog', 'cat']\n",
    "        Output: {'cat': 0.67, 'dog': 0.33}\n",
    "    \"\"\"\n",
    "    probability_dict = {}\n",
    "    total_words = len(word_list)\n",
    "\n",
    "    # Count frequencies\n",
    "    for word in word_list:\n",
    "        probability_dict[word] = probability_dict.get(word, 0.) + 1\n",
    "\n",
    "    # Convert to probabilities\n",
    "    for word, count in probability_dict.items():\n",
    "        probability_dict[word] = count / total_words\n",
    "\n",
    "    return probability_dict\n",
    "\n",
    "\n",
    "# Convert frequency lists to probability distributions\n",
    "for condition_word, next_words in first_order_transitions.items():\n",
    "    first_order_transitions[condition_word] = convert_to_probability_dict(next_words)\n",
    "\n",
    "for condition_pair, next_words in second_order_transitions.items():\n",
    "    second_order_transitions[condition_pair] = convert_to_probability_dict(next_words)\n",
    "\n",
    "\n",
    "def sample_from_distribution(probability_dict):\n",
    "    \"\"\"\n",
    "    CUMULATIVE PROBABILITY SAMPLING METHOD EXPLANATION\n",
    "\n",
    "    Given a discrete probability distribution:\n",
    "    words = {'cat': 0.3, 'dog': 0.5, 'bird': 0.2}\n",
    "\n",
    "    The cumulative probabilities would be:\n",
    "    'cat':  0.0 to 0.3  (0.3)\n",
    "    'dog':  0.3 to 0.8  (0.3 + 0.5)\n",
    "    'bird': 0.8 to 1.0  (0.3 + 0.5 + 0.2)\n",
    "\n",
    "    When we generate a random number between 0 and 1:\n",
    "    - If random = 0.2 → returns 'cat' (falls in 0.0-0.3)\n",
    "    - If random = 0.6 → returns 'dog' (falls in 0.3-0.8)\n",
    "    - If random = 0.9 → returns 'bird' (falls in 0.8-1.0)\n",
    "\n",
    "///////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    VISUAL EXPLANATION OF CUMULATIVE PROBABILITY SAMPLING\n",
    "\n",
    "    1. Original Probabilities:\n",
    "    cat  = 0.3  (30% chance)\n",
    "    dog  = 0.5  (50% chance)\n",
    "    bird = 0.2  (20% chance)\n",
    "\n",
    "    2. We create a \"number line\" from 0 to 1 and divide it into segments:\n",
    "\n",
    "    0.0 -------- 0.3 -------------- 0.8 -------- 1.0\n",
    "         cat          dog              bird\n",
    "    |----30%-----|------50%------|----20%-----|\n",
    "\n",
    "    3. Random number generation acts like throwing a dart at this line:\n",
    "\n",
    "    Example 1: Random number = 0.2\n",
    "    0.0 -------- 0.3 -------------- 0.8 -------- 1.0\n",
    "           ↑\n",
    "          0.2\n",
    "        (picks cat)\n",
    "\n",
    "    Example 2: Random number = 0.6\n",
    "    0.0 -------- 0.3 -------------- 0.8 -------- 1.0\n",
    "                        ↑\n",
    "                       0.6\n",
    "                     (picks dog)\n",
    "\n",
    "    Example 3: Random number = 0.9\n",
    "    0.0 -------- 0.3 -------------- 0.8 -------- 1.0\n",
    "                                         ↑\n",
    "                                        0.9\n",
    "                                     (picks bird)\n",
    "\n",
    "///////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    EXAMPLE OF WHY WE NEED PROBABILITY SAMPLING IN POETRY GENERATION\n",
    "\n",
    "    Let's say we're analyzing this simple poem:\n",
    "    'the cat sat'\n",
    "    'the cat napped'\n",
    "    'the dog sat'\n",
    "\n",
    "    After analysis, our Markov chain would have these probabilities:\n",
    "\n",
    "    1. First word probabilities (initial_probabilities):\n",
    "       'the': 1.0  (appears 3/3 times as first word)\n",
    "\n",
    "    2. First-order transitions (after seeing 'the'):\n",
    "       first_order_transitions['the'] = {\n",
    "           'cat': 0.67,  # 'cat' appears 2/3 times after 'the'\n",
    "           'dog': 0.33   # 'dog' appears 1/3 times after 'the'\n",
    "       }\n",
    "\n",
    "    3. Second-order transitions (after seeing 'the cat'):\n",
    "       second_order_transitions[('the', 'cat')] = {\n",
    "           'sat': 0.5,    # After 'the cat', 'sat' appears 1/2 times\n",
    "           'napped': 0.5  # After 'the cat', 'napped' appears 1/2 times\n",
    "       }\n",
    "\n",
    "////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    Randomly samples a word from a probability distribution\n",
    "    Uses cumulative probability method for sampling\n",
    "\n",
    "    Formula: Find word where cumsum(P(words)) > random(0,1)\n",
    "\n",
    "    Args:\n",
    "        probability_dict (dict): Dictionary of word probabilities\n",
    "    Returns:\n",
    "        str: Sampled word\n",
    "    \"\"\"\n",
    "    random_value = np.random.random()\n",
    "    cumulative_probability = 0.\n",
    "\n",
    "    for word, probability in probability_dict.items():\n",
    "        cumulative_probability += probability\n",
    "        if random_value < cumulative_probability:\n",
    "            return word\n",
    "\n",
    "\n",
    "def select_most_frequent_word(probability_dict):\n",
    "    \"\"\"\n",
    "    Selects the word that appears most frequently in the dictionary.\n",
    "\n",
    "    Example:\n",
    "    If we have probability_dict = {'cat': 0.3, 'dog': 0.5, 'bird': 0.2}\n",
    "    This means:\n",
    "        - 'cat' appears 30% of the time\n",
    "        - 'dog' appears 50% of the time\n",
    "        - 'bird' appears 20% of the time\n",
    "    The function will return 'dog' because 0.5 (50%) is the highest probability\n",
    "\n",
    "    Args:\n",
    "        probability_dict (dict): Dictionary where:\n",
    "            - keys are words\n",
    "            - values are their probabilities\n",
    "    Returns:\n",
    "        str: The word with highest probability\n",
    "    \"\"\"\n",
    "    # Initialize variables to keep track of the most frequent word\n",
    "    highest_probability = 0.0\n",
    "    most_frequent_word = None\n",
    "\n",
    "    # Go through each word and its probability\n",
    "    for word, probability in probability_dict.items():\n",
    "        # If we find a word with higher probability than our current highest\n",
    "        if probability > highest_probability:\n",
    "            # Update our tracking variables\n",
    "            highest_probability = probability\n",
    "            most_frequent_word = word\n",
    "\n",
    "    return most_frequent_word\n",
    "\n",
    "\n",
    "def generate_poetry():\n",
    "    \"\"\"\n",
    "    Generates 4 lines of poetry using the trained Markov model\n",
    "\n",
    "    Generation process uses these probability distributions:\n",
    "    1. First word: P(w1)\n",
    "    2. Second word: P(w2|w1)\n",
    "    3. Subsequent words: P(wt|wt-2,wt-1)\n",
    "    \"\"\"\n",
    "    for _ in range(10):  # Generate 4 lines\n",
    "        line_words = []\n",
    "\n",
    "        # Sample first word from initial distribution\n",
    "        first_word = sample_from_distribution(initial_probabilities)\n",
    "        # first_word = select_most_frequent_word(initial_probabilities)\n",
    "        line_words.append(first_word)\n",
    "\n",
    "        # Sample second word based on first word\n",
    "        second_word = sample_from_distribution(first_order_transitions[first_word])\n",
    "        # second_word = select_most_frequent_word(first_order_transitions[first_word])\n",
    "        line_words.append(second_word)\n",
    "\n",
    "        # Generate rest of the line\n",
    "        while True:\n",
    "            # Get next word based on previous two words\n",
    "            previous_previous = first_word\n",
    "            previous_word = second_word\n",
    "\n",
    "            next_word = sample_from_distribution(second_order_transitions[(previous_previous, previous_word)])\n",
    "            # next_word = select_most_frequent_word(second_order_transitions[(previous_previous, previous_word)])\n",
    "\n",
    "            if next_word == 'END':\n",
    "                break\n",
    "\n",
    "            line_words.append(next_word)\n",
    "\n",
    "            # Shift window of words\n",
    "            first_word = previous_word\n",
    "            second_word = next_word\n",
    "\n",
    "        print(' '.join(line_words))\n",
    "\n",
    "\n",
    "# Generate poetry\n",
    "generate_poetry()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "սևուկ ուլիկ\n",
      "«բարի օր»— ասաց աղքատն ու պատմեց սոված գայլի սիրուն աղջկա ու չոր ծառի ապսպրանքը։\n",
      "գիշերը դարձյալ դերվիշի շոր մտավ հարուն ալ ռաշիդ թագավորը։ հարուն ալ ռաշիդ թագավորը սովորություն ուներ՝ շորերը փոխած ման էր գալիս իմանալու թե ինչ պատասխան տանք։ հիմի մեզ ինչ անի մենք ենք մեղավոր վայ թե քեզ հետ կենամ բաժանվում եմ իմ բաժինը տուր գնամ ջոկ ապրեմ։\n",
      "— որը կպատահի։\n",
      "էս վախկոտ նազարը մի անշնորհք ու ալարկոտ մարդ է պատահում։ էս ծեր մարդը մի քանի հասցրի ինչ ուներ՝ առաջիս փռեց։ իմ տասը մանեթը տուր մնացածը քու փողն է ընչի՞ս է պետք\n",
      "— վա՜յ— ասում է— էս էլ քո սիրած քույրը տանը ինչ ունեինքչունեինք՝ ջարդեց։\n",
      "մարդը կնկանն է ասում հիմար կնիկը մարդուն ու միշտ կռվելիս են լինում։ ախպերը պսակվում է կնիկ է բերում։\n",
      "գալիս են ի՞նչ ես միտք անում— հարցնում է թագավորը։\n",
      "լուսադեմին աքլորը կանչեց՝ ծուղրուղո՜ւ։\n",
      "— ուրեմն արծիվը ճուտ է եղել։\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
